<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Solutions for Microsoft Azure Stack â€“ Layers Below S2D Stack</title>
    <link>https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/</link>
    <description>Recent content in Layers Below S2D Stack on Solutions for Microsoft Azure Stack</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	  <atom:link href="https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Storage Devices</title>
      <link>https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/</guid>
      <description>
        
        
        &lt;!-- TOC --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#storage-devices&#34;&gt;Storage Devices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#interfaces&#34;&gt;Interfaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#storage-protocol&#34;&gt;Storage Protocol&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#storage-configurations&#34;&gt;Storage Configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#os-disks&#34;&gt;OS Disks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#consumer-grade-ssds&#34;&gt;Consumer-Grade SSDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#exploring-stack-with-powershell&#34;&gt;Exploring Stack with PowerShell&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#get-physicaldisk&#34;&gt;Get-PhysicalDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#storage-reliability-counter&#34;&gt;Storage Reliability Counter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/#performance-results&#34;&gt;Performance results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /TOC --&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/Stack-PhysicalDisks.png
&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;p&gt;Microsoft Documentation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure-stack/hci/concepts/choose-drives&#34;&gt;https://learn.microsoft.com/en-us/azure-stack/hci/concepts/choose-drives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure-stack/hci/concepts/drive-symmetry-considerations&#34;&gt;https://learn.microsoft.com/en-us/azure-stack/hci/concepts/drive-symmetry-considerations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVMe vs SATA&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sata-io.org/sites/default/files/documents/NVMe%20and%20AHCI%20as%20SATA%20Express%20Interface%20Options%20-%20Whitepaper_.pdf&#34;&gt;https://sata-io.org/sites/default/files/documents/NVMe%20and%20AHCI%20as%20SATA%20Express%20Interface%20Options%20-%20Whitepaper_.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nvmexpress.org/wp-content/uploads/2013/04/IDF-2012-NVM-Express-and-the-PCI-Express-SSD-Revolution.pdf&#34;&gt;http://www.nvmexpress.org/wp-content/uploads/2013/04/IDF-2012-NVM-Express-and-the-PCI-Express-SSD-Revolution.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nvmexpress.org/wp-content/uploads/NVMe-101-1-Part-2-Hardware-Designs_Final.pdf&#34;&gt;https://nvmexpress.org/wp-content/uploads/NVMe-101-1-Part-2-Hardware-Designs_Final.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nvmexpress.org/wp-content/uploads/NVMe_Infrastructure_final1.pdf&#34;&gt;https://nvmexpress.org/wp-content/uploads/NVMe_Infrastructure_final1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.storagereview.com/review/dell-emc-poweredge-r750-hands-on&#34;&gt;https://www.storagereview.com/review/dell-emc-poweredge-r750-hands-on&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.dell.com/manuals/common/dellemc-nvme-io-topologies-poweredge.pdf&#34;&gt;https://dl.dell.com/manuals/common/dellemc-nvme-io-topologies-poweredge.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.servethehome.com/dell-emc-poweredge-r7525-review-flagship-dell-dual-socket-server-amd-epyc/&#34;&gt;https://www.servethehome.com/dell-emc-poweredge-r7525-review-flagship-dell-dual-socket-server-amd-epyc/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interfaces&#34;&gt;Interfaces&lt;/h2&gt;
&lt;p&gt;While SATA is still well performing for most of the customers (see performance results), NVMe offers benefit of higher capacity and also more effective protocol (AHCI vs NVMe), that was developed specifically for SSDs (opposite to AHCI, that was developed for spinning media). SATA/SAS is however not scaling well with the larger disks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/ScalablePerformance.png
&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source: &lt;a href=&#34;https://nvmexpress.org/wp-content/uploads/NVMe-101-1-Part-2-Hardware-Designs_Final.pdf&#34;&gt;https://nvmexpress.org/wp-content/uploads/NVMe-101-1-Part-2-Hardware-Designs_Final.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also another aspect of performance limitation of SATA/SAS devices - the controller. All SATA/SAS devices are connected to one SAS controller (non-raid) that has limited speed (only one PCI-e connection).&lt;/p&gt;
&lt;p&gt;Drive Connector is universal (U2, also known as SFF-8639)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/DriveConnector.png
&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source: &lt;a href=&#34;https://nvmexpress.org/wp-content/uploads/NVMe_Infrastructure_final1.pdf&#34;&gt;https://nvmexpress.org/wp-content/uploads/NVMe_Infrastructure_final1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVMe drives are mapped directly to CPU&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/NVMeDriveMapping.png
&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source: &lt;a href=&#34;https://dl.dell.com/manuals/common/dellemc-nvme-io-topologies-poweredge.pdf&#34;&gt;https://dl.dell.com/manuals/common/dellemc-nvme-io-topologies-poweredge.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVMe backplane connection - Example AX7525 - 16 PCIe Gen4 lanes in each connection (8 are used), 12 connections in backplane, in this case no PCIe switches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX7525Backplane.png
&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source: &lt;a href=&#34;https://www.servethehome.com/dell-emc-poweredge-r7525-review-flagship-dell-dual-socket-server-amd-epyc/dell-emc-poweredge-r7525-internal-view-24x-nvme-backplane-and-fans/&#34;&gt;https://www.servethehome.com/dell-emc-poweredge-r7525-review-flagship-dell-dual-socket-server-amd-epyc/dell-emc-poweredge-r7525-internal-view-24x-nvme-backplane-and-fans/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;storage-protocol&#34;&gt;Storage Protocol&lt;/h2&gt;
&lt;p&gt;SSDs were originally created to replace conventional rotating media. As such they were designed to connect to the same bus types as HDDs, both SATA and SAS (Serial ATA and Serial Attached SCSI).&lt;/p&gt;
&lt;p&gt;However, this imposed speed limitations on the SSDs.  Now a new type of SSD exists that attaches to PCI-e. Known as NVMe SSDs or simply NVMe.&lt;/p&gt;
&lt;p&gt;For 1M IOPS, NVMe has more than &lt;a href=&#34;https://www.nvmexpress.org/wp-content/uploads/2013/04/IDF-2012-NVM-Express-and-the-PCI-Express-SSD-Revolution.pdf&#34;&gt;50% less latency with less than 50% CPU Cycles used&lt;/a&gt;. It is due to &lt;a href=&#34;https://en.wikipedia.org/wiki/NVM_Express#Comparison_with_AHCI&#34;&gt;improved protocol&lt;/a&gt; (AHCI vs NVMe)&lt;/p&gt;
&lt;h2 id=&#34;storage-configurations&#34;&gt;Storage Configurations&lt;/h2&gt;
&lt;p&gt;(slowest to fastest)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hybrid (HDD+SSD)&lt;/li&gt;
&lt;li&gt;All Flash (All SSD)&lt;/li&gt;
&lt;li&gt;NVMe+HDD&lt;/li&gt;
&lt;li&gt;All-NVMe&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When combining multiple media types, faster media will be used as caching. While it is recommended to use 10% of the capacity for cache, it should be noted, that it is just important to not spill the cache with the production workload, as it will dramatically reduce performance. Therefore all production workload should fit into the Storage Bus Layer Cache (cache devices). The sweet spot (price vs performance) is combination of fast NVMe (mixed use or write intensive) with HDDs. For performance intensive workloads it&amp;rsquo;s recommended to use all-flash solutions as caching introduces ~20% overhead + less predicable behavior (data can be already destaged&amp;hellip;), therefore it is recommended to use All-Flash for SQL workloads.&lt;/p&gt;
&lt;p&gt;Performance drop when spilling cache devices:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/CachePerfDrop.png
&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source: &lt;a href=&#34;https://web.archive.org/web/20160817193242/http://itpeernetwork.intel.com/iops-performance-nvme-hdd-configuration-windows-server-2016-storage-spaces-direct/&#34;&gt;https://web.archive.org/web/20160817193242/http://itpeernetwork.intel.com/iops-performance-nvme-hdd-configuration-windows-server-2016-storage-spaces-direct/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;os-disks&#34;&gt;OS Disks&lt;/h2&gt;
&lt;p&gt;In Dell Servers are BOSS (Boot Optimized Storage Solution) cards used. In essence it card wih 2x m2 2280 NVMe disks connected to PCI-e with configurable non-RAID/RAID 1&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX750BOSS01.png
&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX750BOSS02.png
&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;consumer-grade-ssds&#34;&gt;Consumer-Grade SSDs&lt;/h2&gt;
&lt;p&gt;You should avoid any consumer grade SSDs as consumer grade SSDs might contain NAND with higher latency (therefore there can be performance drop after spilling FTL buffer) or because consumer grade SSDs are not power protected (PLP). You can learn more about why consumer-grade SSDs are not good idea in a &lt;a href=&#34;https://techcommunity.microsoft.com/t5/storage-at-microsoft/don-t-do-it-consumer-grade-solid-state-drives-ssd-in-storage/ba-p/425914&#34;&gt;blog post&lt;/a&gt;. Consumer-grade SSDs do also have lower DWPD (Disk Written Per Day). You can learn about DWPD in &lt;a href=&#34;https://blogs.technet.microsoft.com/filecab/2017/08/11/understanding-dwpd-tbw/&#34;&gt;this blogpost&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;exploring-stack-with-powershell&#34;&gt;Exploring Stack with PowerShell&lt;/h2&gt;
&lt;h3 id=&#34;get-physicaldisk&#34;&gt;Get-PhysicalDisk&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;$Server = &amp;quot;axnode1&amp;quot;
Get-PhysicalDisk -CimSession $Server | Format-Table -Property FriendlyName, Manufacturer, Model, SerialNumber, MediaType, BusType, SpindleSpeed, LogicalSectorSize, PhysicalSectorSize
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/PowerShell01.png
&#34;&gt;&lt;/p&gt;
&lt;p&gt;From screenshot you can see, that AX640 BOSS card reports as SATA device with Unspecified Mediatype, while SAS disks are reported as SSDs, with SAS BusType. Let&amp;rsquo;s deep dive into BusType/MediaType a little bit (see table below)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/BusType.png
&#34;&gt;&lt;/p&gt;
&lt;p&gt;Storage Spaces requires BusType SATA/SAS/NVMe or SCM. BusType RAID is unsupported.&lt;/p&gt;
&lt;p&gt;You can also see Logical Sector Size and Physical Sector size. This refers to Drive Type (4K native vs 512E vs 512).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&amp;ldquo;LogicalSectorSize&amp;rdquo; value&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&amp;ldquo;PhysicalSectorSize&amp;rdquo; value&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Drive type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4096&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4096&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4K native&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;512&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4096&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Advanced Format (also known as 512E)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;512&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;512&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;512-byte native&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Reference&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-US/troubleshoot/windows-server/backup-and-storage/support-policy-4k-sector-hard-drives&#34;&gt;https://learn.microsoft.com/en-US/troubleshoot/windows-server/backup-and-storage/support-policy-4k-sector-hard-drives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/hh147334(v=ws.10)?redirectedfrom=MSDN&#34;&gt;https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/hh147334(v=ws.10)?redirectedfrom=MSDN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;storage-reliability-counter&#34;&gt;Storage Reliability Counter&lt;/h3&gt;
&lt;p&gt;Once disk is added to storage spaces, S.M.A.R.T. attributes can be filtered out. For reading disk status (such as wear level temperatures&amp;hellip;) can be get-storagereliability counter used.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;$Server = &amp;quot;axnode1&amp;quot;
Get-PhysicalDisk -CimSession $Server | Get-StorageReliabilityCounter -CimSession $Server | Format-Table -Property DeviceID, Wear, Temperature*, PowerOnHours, ManufactureDate, ReadLatencyMax, WriteLatencyMax, PSComputerName
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;

https://wturner123.github.io/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/PowerShell02.png
&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;performance-results&#34;&gt;Performance results&lt;/h2&gt;
&lt;p&gt;From the results below you can see that SATA vs SAS vs NVMe is 590092 vs 738507 vs 1496373 4k 100% read IOPS. All measurements were done with VMFleet 2.0 &lt;a href=&#34;https://github.com/DellGEOS/AzureStackHOLs/tree/main/lab-guides/05-TestPerformanceWithVMFleet&#34;&gt;https://github.com/DellGEOS/AzureStackHOLs/tree/main/lab-guides/05-TestPerformanceWithVMFleet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The difference between SAS and SATA is also 8 vs 4 disks in each node. The difference between SAS and NVMe is more than double.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX6515-All-Flash-8SATA-SSDs-32VMs-2Node.txt&#34;&gt;AX6515 - 2nodes, 16 cores and 4xSATA SSDs each&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX6515-All-Flash-8SATA-SSDs-32VMs-2Node-SC-Dedup.txt&#34;&gt;AX6515 - 2nodes, 16 cores and 4xSATA SSDs each, secured core and deduplication enabled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX6515-All-Flash-8SATA-SSDs-32VMs-2Node-SC.txt&#34;&gt;AX6515 - 2nodes, 16 cores and 4xSATA SSDs each, secured core enabled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX6515-All-Flash-8SATA-SSDs-32VMs-2Node-SC-BitLocker.txt&#34;&gt;AX6515 - 2nodes, 16 cores and 4xSATA SSDs each, secured core &amp;amp; BitLocker enabled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/AX6515-All-Flash-16SAS-SSDs-32VMs-2nodes-azshci.txt&#34;&gt;AX6515 - 2nodes, 16 cores and 8xSAS SSDs each&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wturner123.github.io/azurestack-docs/azurestack-docs/docs/hci/storagestack/01-layers-below-s2d-stack/01-storagedevices/R640-All-NVMe-16NVMe-64VMs-2Node.txt&#34;&gt;R640 - 2nodes, 32 cores and 8xNVMe SSDs each&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
